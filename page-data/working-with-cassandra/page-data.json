{"componentChunkName":"component---src-templates-blog-post-js","path":"/working-with-cassandra/","result":{"data":{"site":{"siteMetadata":{"title":"Backtracking Dev","siteUrl":"https://backtracking.dev"}},"markdownRemark":{"id":"763f16a6-63a0-5b44-b4e4-8b20a09759b7","excerpt":"While working with cassandra we experimented a lot to analyse the performance of the cassandra on kubernetes cluster.  Here are the some learnings from the…","html":"<p>While working with cassandra we experimented a lot to analyse the performance of the cassandra on kubernetes cluster. </p>\n<p>Here are the some learnings from the extensive testing.</p>\n<h3>Write (Bulk)</h3>\n<ul>\n<li>\n<p>[k8s] Concurrency: 200; Num CPU’s Client: 2; # Records: 116547279</p>\n<ul>\n<li>Time taken to Insert in C* 2h54m26.765794644s</li>\n<li>Total Record Counts: 173801976</li>\n<li>Error Counts: 0</li>\n</ul>\n</li>\n<li>\n<p>[k8s] Concurrency: 1000; Num CPU’s Client: 16; # Records: 116547186</p>\n<ul>\n<li>Time taken to Insert in C* 1h56m43.787323892s</li>\n<li>Total Record Counts: 173801976</li>\n<li>Error Counts: 11449</li>\n</ul>\n</li>\n<li>\n<p>Using spark to import data directly from s3</p>\n<ul>\n<li>Number of nodes in cassandra cluster: 8 x i3.xlarge</li>\n<li>Replication factor: 2</li>\n<li>Number of nodes in emr cluster: 10 x m3.xlarge</li>\n<li>Number of records: 4.820.489.450 (all snippets)</li>\n<li>Time taken: 24 hours</li>\n<li>Number of writes per core per second: 1743</li>\n</ul>\n</li>\n</ul>\n<p><strong>Inference</strong>: Client scales linearly with the number of available cores. We need to make sure that we are not keeping the concurrency too high. Also in the second case the number of available cpu’s with the client is 16 this is greater than the number of cpus with all cassandra nodes (4 x 3 = 12)</p>\n<h3>Read (Bulk)</h3>\n<ul>\n<li>\n<p><strong>Test setup 1</strong></p>\n<ul>\n<li>Number of nodes in cassandra cluster: 6 x i3.xlarge</li>\n<li>Number of client nodes: 2 x m3.xlarge</li>\n<li>Number of queries: ~21M</li>\n<li>Time taken: ~7000 sec</li>\n<li>Number of reads per cassandra core per second: 125</li>\n<li>Number of reads per client core per second: 375</li>\n</ul>\n</li>\n</ul>\n<h3>Test: Does commit log placement affect write performance?</h3>\n<ul>\n<li>\n<p><strong>Test setup: cassandra cluster running on 3 x i3.xlarge</strong></p>\n<ul>\n<li>Number of client writer nodes: 10 x m3.xlarge</li>\n<li>Number of rows to insert: ~46M</li>\n<li>\n<p>Commit log placements tested:</p>\n<ul>\n<li>Ephemeral drive, same as where data is stored</li>\n<li>Root SSD drive</li>\n<li>Separate EBS (HDD Standard)</li>\n<li>Separate EBS (SSD GP2)\nTime taken in each test: 12 minutes</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<h3>Failure Testing</h3>\n<h3>Case 1: Purge all the pods but instances with data persists</h3>\n<p>On restarting we need to make sure that we remove all files in the path: /var/lib/cassandra/data/system/*</p>\n<p>If not done the last cluster state causes the second pod to enter a crash loop backoff and it causes problems for cluster to start.</p>\n<p>Performing this operation still has no impact on data. Remember we are only deleting data in the system keyspace, the rest of the keyspace data is still kept intact.</p>\n<p>Note: This should only be done when the entire cluster was purged and previous data still persists.</p>\n<h3>Case 2: Delete a cluster pod; Instance and data persists.</h3>\n<p>This case has two scenarios:</p>\n<ol>\n<li>Deleted pod is a seed node.</li>\n<li>Deleted pod is a non-seed node.\nIn both the cases deleting the pod has no affect in availability of data from cluster. We can delete the pod and on the same instance the same pod starts (stateful-set construct at work in k8s) and it starts up properly connecting back to the ring in the cluster.</li>\n</ol>\n<h3>Case 3: Scale up the cluster</h3>\n<p>Generally, if you start some new nodes with proper configuration values (mainly seeds and cluster_name), they should eventually join the cluster and then receive their portion of data from other nodes. This process is called bootstrapping.</p>\n<p>However, if two or more nodes try to bootstrap at the same time, one might fail to proceed until the other one has finished his job. In logs, you might find corresponding exceptions:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">Exception (java.lang.UnsupportedOperationException) encountered during startup:\nOther bootstrapping/leaving/moving nodes detected, cannot bootstrap while cassandra.consistent.rangemovement is true</code></pre></div>\n<p>But as said, this is not critical issue: cassandra will retry bootstrapping over and over again until succeeded.\nMore details discussed in depth in <a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html\">this article</a></p>\n<h3>Case 4: Remove or downscale the cluster nodes</h3>\n<ol>\n<li>Run nodetool drain &#x26;&#x26; nodetool stopdaemon on node planned for removal</li>\n<li>Run nodetool status on any other node in the cluster. Make sure that the victim node is marked as DN</li>\n<li>Run nodetool removenode <GUID> with victim’s GUID. This command will take some time.</li>\n<li>To remove all nodes that are down, run the same stuff in a loop, like:\nfor node in <code class=\"language-text\">nodetool status | grep ^DN | awk {&#39;print $7&#39;}</code>; do nodetool removenode $node; done</li>\n</ol>\n<h3>Case 5: Replacing dead node</h3>\n<p>Can be achieved by:</p>\n<ol>\n<li>removing dead node as described in Case 6</li>\n<li>starting a new node as described in Case 5\nHowever, there is more efficient way possible to add <a href=\"http://thelastpickle.com/blog/2017/05/23/auto-bootstrapping-part1.html#adding-a-replacement-node\">a replacement node</a></li>\n</ol>\n<h3>Case 6: What happens when disk becomes full</h3>\n<p>This section explains what kinds of issues might arise when the disk where cassandra data folder resides, becomes full.</p>\n<p>The main thing we should know is that cassandra does not consider this case as critical, so if node is unable to do a planned background compaction, it won’t just do it. As far as there is enough space in commitlog pool, all data writes will still be stored in commitlog. However, commitlog maximum size is controlled by commitlog<em>total</em>space<em>in</em>mb config property, and by default its set to several gigabytes. For performance reasons, its not recommended to increase this value too much. So if you keep writing new data, commitlog will also get full.</p>\n<p>After that, all writes to this node will fail, while node will still try to stay up and running. In particular, nodetool status will still report node as UN (up normal).</p>\n<p>Depending on cluster configuration, replication<em>factor property and desired consistency level, a client who is doing writes to the cluster might not notice any issue. So for example if we have more than 1 node in cluster, use replication</em>factor = 2 and consistency ONE, any write request will be successful, because second replica will always successfully go to another, healthy node.</p>\n<p>But in practice things may work differently. As of cassandra 3.11.2, such an overloaded node seems to work quite unstable, so over time service may die with following log messages:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">java.lang.OutOfMemoryError: Java heap space\nDumping heap to java_pid2795.hprof ...\nUnable to create java_pid2795.hprof: Permission denied\n\n    java.lang.OutOfMemoryError: Java heap space\n -XX:OnOutOfMemoryError=&quot;kill -9 %p&quot;\n   Executing /bin/sh -c &quot;kill -9 2795&quot;...\nbash: line 1:  2795 Killed                  docker-entrypoint.sh</code></pre></div>\n<p>After restarting the service, while there is still no free space left on device, cassandra might behave unpredictably. For example, some subservices would start normally, and others not. But if you first free up some space and then restart it again, cassandra will start normally.\nTherefore, you need to make sure you have some smart policy for service healthcheck and restarting.</p>\n<h3>Case 7: <a href=\"https://opencredo.com/cassandra-tombstones-common-issues/\">Tombstones: common issues</a></h3>","frontmatter":{"title":"Working with cassandra at scale","tags":["cassandra","performance","nosql","kubernetes"],"date":"September 06, 2020","description":"Summary of learnings while testing cassandra at scale","author":"abhi","featuredImage":{"childImageSharp":{"fluid":{"base64":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAIAAABM9SnKAAAACXBIWXMAAAsSAAALEgHS3X78AAAAsklEQVQY042OvwuCQBiG+6vb+h80aKumhixcCmyIhoIyD3/k3Wk0RFFTlskdkh7qFeIkGvWs7/e879d4f4NnD98/Xa5P78YY45xXTxq1YhAE0nrXGkybfVkcKYLYvnteXliuKMlF9gpD6B5626OwxF393FHUoTSu3ahZztKUEoKwM1ms5NncMow0Sf6VWRw7OdgydKBugKYBAAghP94uiKIIIQQhNE3Ttu2962KEKKVV+QNs9VDshbRuMwAAAABJRU5ErkJggg==","aspectRatio":3.392330383480826,"src":"/static/bd30e8ad5b7a65638af60a40cede3526/a54c6/cassandra.png","srcSet":"/static/bd30e8ad5b7a65638af60a40cede3526/59beb/cassandra.png 200w,\n/static/bd30e8ad5b7a65638af60a40cede3526/c0bbd/cassandra.png 400w,\n/static/bd30e8ad5b7a65638af60a40cede3526/a54c6/cassandra.png 800w,\n/static/bd30e8ad5b7a65638af60a40cede3526/9911c/cassandra.png 1200w,\n/static/bd30e8ad5b7a65638af60a40cede3526/c8bd7/cassandra.png 1600w,\n/static/bd30e8ad5b7a65638af60a40cede3526/37945/cassandra.png 2300w","sizes":"(max-width: 800px) 100vw, 800px"}}}}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/working-with-cassandra/","previous":{"fields":{"slug":"/confluent-salesforce-connector/"},"frontmatter":{"author":"pari","tags":["kafka","salesforce","confluent","kafka-connect"],"title":"Salesforce Connector for Confluent Platform"}},"next":{"fields":{"slug":"/terraform-aws-elasticsearch/aws-elasticsearch/"},"frontmatter":{"author":"pari","tags":["aws","terraform","elasticsearch","cognito","nginx"],"title":"Terraform script for Amazon Elasticsearch Service with Cognito authentication"}}}}}